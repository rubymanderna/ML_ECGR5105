{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubymanderna/ML_ECGR5105/blob/main/Assignment_7/Homework7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h3y6rEIBWNyJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1 (50pts):\n",
        "a. Build a Convolutional Neural Network, like what we built in lectures to classify the images across all 10 classes in CIFAR 10. You need to adjust the fully connected layer at the end properly concerning the number of output classes. Train your network for 300 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare them against a fully connected network (homework 2) on training time, achieved accuracy, and model size.\n",
        "Make sure to submit your code by providing the GitHub URL of your course repository for this course.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ChXiD6GJaVol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with modifications\n",
        "num_epochs = 300\n",
        "early_stopping_threshold = 10\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            validation_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    # Print and check for early stopping\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    average_validation_loss = validation_loss / len(test_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Validation Loss: {average_validation_loss}\")\n",
        "\n",
        "    if average_validation_loss < best_validation_loss:\n",
        "        best_validation_loss = average_validation_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_threshold:\n",
        "        print(f\"Early stopping after {early_stopping_threshold} epochs without improvement.\")\n",
        "        break\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Training Time: {num_epochs} epochs, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Nwiwn0lEiC68",
        "outputId": "11e81171-2b41-4196-d13c-aebaaac153f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 1.361691037940857, Validation Loss: 1.0899869195974556\n",
            "Epoch 2/300, Loss: 0.9794067739297057, Validation Loss: 0.9373889838813976\n",
            "Epoch 3/300, Loss: 0.8099364701591795, Validation Loss: 0.9106965148524874\n",
            "Epoch 4/300, Loss: 0.699265246462944, Validation Loss: 0.8195717889032547\n",
            "Epoch 5/300, Loss: 0.5885692591709859, Validation Loss: 0.8635666307750022\n",
            "Epoch 6/300, Loss: 0.49118287442132946, Validation Loss: 0.8391760082761194\n",
            "Epoch 7/300, Loss: 0.4010898284137706, Validation Loss: 0.8895851544513824\n",
            "Epoch 8/300, Loss: 0.3176781895387051, Validation Loss: 1.0276734631532316\n",
            "Epoch 9/300, Loss: 0.24845535959810247, Validation Loss: 1.079126067601951\n",
            "Epoch 10/300, Loss: 0.19249336764006816, Validation Loss: 1.192319303561168\n",
            "Epoch 11/300, Loss: 0.14785454512271276, Validation Loss: 1.3344713544390003\n",
            "Epoch 12/300, Loss: 0.11291855411685031, Validation Loss: 1.5035629325611577\n",
            "Epoch 13/300, Loss: 0.10448773882811523, Validation Loss: 1.5770150187668528\n",
            "Epoch 14/300, Loss: 0.08390522266135496, Validation Loss: 1.7134962480538969\n",
            "Early stopping after 10 epochs without improvement.\n",
            "Training Time: 300 epochs, Accuracy: 0.7029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Homework 2  results or simple Neural network\n",
        "With one hidden layer:\n",
        "Training Time: 447.68 seconds\n",
        "Training Loss: 1.4940\n",
        "Validation Accuracy: 0.4517\n",
        "Test Accuracy: 0.4517\n",
        "\n",
        "compare with this simple CNN\n",
        "\n",
        "Epoch 14/300, Loss: 0.08390522266135496, Validation Loss: 1.7134962480538969\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 300 epochs, Accuracy: 0.7029\n",
        "\n",
        "Comparing these results, the simple CNN achieved a higher accuracy (70.29%) compared to the neural network (45.17%) on the validation set. Additionally, the CNN had a lower training loss, indicating better convergence during training. The early stopping mechanism was triggered after 10 epochs without improvement in validation loss.\n",
        "\n",
        "In summary, the simple CNN seems to outperform the neural network based on the provided metrics.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JPOzpOBobjxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "b. Extend your CNN by adding one more additional convolution layer followed by an activation function and pooling function. You also need to adjust your fully connected layer properly with respect to intermediate feature dimensions. Train your network for 300 epochs. Report your training time, loss, and evaluation accuracy after 300 epochs.\n",
        "Analyze your results in your report and compare your model size and accuracy over the baseline implementation in Problem 1.a. Do you see any over-fitting? Make sure to submit your code by providing the GitHub URL of your course repository for this course."
      ],
      "metadata": {
        "id": "AsEXgYcgB6cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the extended CNN architecture\n",
        "class ExtendedCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ExtendedCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 256)  # Adjusted fully connected layer\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)  # Adjusted view based on the dimensions\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = ExtendedCNN(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with modifications for early stopping\n",
        "num_epochs = 300\n",
        "early_stopping_threshold = 10\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            validation_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    # Print and check for early stopping\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    average_validation_loss = validation_loss / len(test_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Validation Loss: {average_validation_loss}\")\n",
        "\n",
        "    if average_validation_loss < best_validation_loss:\n",
        "        best_validation_loss = average_validation_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_threshold:\n",
        "        print(f\"Early stopping after {early_stopping_threshold} epochs without improvement.\")\n",
        "        break\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Training Time: {num_epochs} epochs, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "8Q_eme68WSFW",
        "outputId": "319f695d-600d-489c-841b-f5dcac353f75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 1.359804761150609, Validation Loss: 1.0365461164219365\n",
            "Epoch 2/300, Loss: 0.923181984849903, Validation Loss: 0.8744377083839125\n",
            "Epoch 3/300, Loss: 0.7380762781633441, Validation Loss: 0.7795194197612204\n",
            "Epoch 4/300, Loss: 0.6055254779965676, Validation Loss: 0.7402167462619247\n",
            "Epoch 5/300, Loss: 0.5048171597559129, Validation Loss: 0.782935691866905\n",
            "Epoch 6/300, Loss: 0.4096251341330883, Validation Loss: 0.7575188702458788\n",
            "Epoch 7/300, Loss: 0.32877071233242366, Validation Loss: 0.8034545285686566\n",
            "Epoch 8/300, Loss: 0.2531358630341642, Validation Loss: 0.8860977834956661\n",
            "Epoch 9/300, Loss: 0.1947809270752208, Validation Loss: 0.9268578143825956\n",
            "Epoch 10/300, Loss: 0.15679256678761347, Validation Loss: 1.0980416898894463\n",
            "Epoch 11/300, Loss: 0.12297829178631153, Validation Loss: 1.2072431223028024\n",
            "Epoch 12/300, Loss: 0.11625367859044991, Validation Loss: 1.2491631274390373\n",
            "Epoch 13/300, Loss: 0.0926953430473328, Validation Loss: 1.311971895064518\n",
            "Epoch 14/300, Loss: 0.08758060977189228, Validation Loss: 1.3732742405241463\n",
            "Early stopping after 10 epochs without improvement.\n",
            "Training Time: 300 epochs, Accuracy: 0.761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "problem 1 b\n",
        "\n",
        " Modified CNN -by adding one more additional convolution layer followed by an activation function and pooling function.\n",
        "\n",
        "Epoch 14/300, Loss: 0.08758060977189228, Validation Loss: 1.3732742405241463\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 300 epochs, Accuracy: 0.761\n",
        "\n",
        "Earlier simeple CNN output\n",
        "\n",
        "Epoch 14/300, Loss: 0.08390522266135496, Validation Loss: 1.7134962480538969\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 300 epochs, Accuracy: 0.7029\n",
        "\n",
        "comparision Comparing these results:\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Modified CNN: Accuracy of 76.1%\n",
        "Simple CNN: Accuracy of 70.29%\n",
        "The modified CNN achieved a higher accuracy compared to the simple CNN.\n",
        "\n",
        "Training Loss:\n",
        "\n",
        "Modified CNN: Loss of 0.0876\n",
        "Simple CNN: Loss of 0.0839\n",
        "The training loss of the modified CNN is slightly higher than that of the simple CNN.\n",
        "\n",
        "Validation Loss:\n",
        "\n",
        "Modified CNN: Validation loss of 1.3733\n",
        "Simple CNN: Validation loss of 1.7135\n",
        "The modified CNN has a lower validation loss compared to the simple CNN, indicating better generalization.\n",
        "\n",
        "Training Time:\n",
        "\n",
        "Both models were trained for 300 epochs, and early stopping was triggered after 10 epochs without improvement.\n",
        "\n",
        " the modified CNN with an additional convolution layer, activation function, and pooling function achieved higher accuracy and a lower validation loss compared to the simple CNN.\n"
      ],
      "metadata": {
        "id": "imN3YVQ4d11S"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cRtXcpFOaq-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 (50pts)\n",
        "a. Build a ResNet-based Convolutional Neural Network, like what we built in lectures (with skip connections), to classify the images across all 10 classes in CIFAR 10. For this problem, let's use 10 blocks for ResNet and call it ResNet-10. Use similar dimensions and channels as we need in lectures.\n",
        "Train your network for 300 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare them against problem 1.b on training time, achieved accuracy, and model size.\n",
        "Make sure to submit your code by providing the GitHub URL of your course repository for this course.\n"
      ],
      "metadata": {
        "id": "rW70oXRyalzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-10 architecture\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(16, 16, num_blocks=2, stride=1)\n",
        "        self.layer2 = self.make_layer(16, 32, num_blocks=2, stride=2)\n",
        "        self.layer3 = self.make_layer(32, 64, num_blocks=2, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the ResNet-10 model, loss function, and optimizer\n",
        "model = ResNet10(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop with modifications for early stopping\n",
        "num_epochs = 300\n",
        "early_stopping_threshold = 10\n",
        "best_validation_loss = float('inf')\n",
        "early_stopping_counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    validation_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "\n",
        "\n",
        "     for inputs, labels in test_loader:\n",
        "       inputs, labels = inputs.to(device), labels.to(device)\n",
        "       outputs = model(inputs)\n",
        "       validation_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    # Print and check for early stopping\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    average_validation_loss = validation_loss / len(test_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Validation Loss: {average_validation_loss}\")\n",
        "\n",
        "    if average_validation_loss < best_validation_loss:\n",
        "        best_validation_loss = average_validation_loss\n",
        "        early_stopping_counter = 0\n",
        "    else:\n",
        "        early_stopping_counter += 1\n",
        "\n",
        "    if early_stopping_counter >= early_stopping_threshold:\n",
        "        print(f\"Early stopping after {early_stopping_threshold} epochs without improvement.\")\n",
        "        break\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Training Time: {num_epochs} epochs, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fCr8-sAsalij",
        "outputId": "b8be4019-25bf-4576-904d-fde5b9a694ab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 1.3188096072210376, Validation Loss: 1.1471581941197633\n",
            "Epoch 2/300, Loss: 0.9111275082963812, Validation Loss: 0.8510392513244774\n",
            "Epoch 3/300, Loss: 0.7477234565007412, Validation Loss: 0.7468047850071244\n",
            "Epoch 4/300, Loss: 0.6438134951359781, Validation Loss: 0.6981770506330357\n",
            "Epoch 5/300, Loss: 0.5668380812305929, Validation Loss: 0.7218952287154593\n",
            "Epoch 6/300, Loss: 0.5071670342131954, Validation Loss: 0.6313433089074055\n",
            "Epoch 7/300, Loss: 0.4576592723960462, Validation Loss: 0.5929360507400172\n",
            "Epoch 8/300, Loss: 0.41199194761874425, Validation Loss: 0.5576162442659877\n",
            "Epoch 9/300, Loss: 0.37443249370626475, Validation Loss: 0.638221031920925\n",
            "Epoch 10/300, Loss: 0.3364449573390167, Validation Loss: 0.6587496407472404\n",
            "Epoch 11/300, Loss: 0.30171978665168026, Validation Loss: 0.6108303006477417\n",
            "Epoch 12/300, Loss: 0.26873460459663434, Validation Loss: 0.687041980161029\n",
            "Epoch 13/300, Loss: 0.24047756902016032, Validation Loss: 0.6525346446948447\n",
            "Epoch 14/300, Loss: 0.21624734852453478, Validation Loss: 0.6209869627739973\n",
            "Epoch 15/300, Loss: 0.19553211190835443, Validation Loss: 0.6488805641034606\n",
            "Epoch 16/300, Loss: 0.17149349159139501, Validation Loss: 0.6609803588147376\n",
            "Epoch 17/300, Loss: 0.15586764125339211, Validation Loss: 0.7480074787975117\n",
            "Epoch 18/300, Loss: 0.14074797088475635, Validation Loss: 0.7536989632685474\n",
            "Early stopping after 10 epochs without improvement.\n",
            "Training Time: 300 epochs, Accuracy: 0.8054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 a answer -\n",
        "Below is modified CNN output\n",
        "\n",
        "Epoch 14/300, Loss: 0.08758060977189228, Validation Loss: 1.3732742405241463\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 300 epochs, Accuracy: 0.761\n",
        "\n",
        "ResNET output\n",
        "\n",
        "Epoch 18/300, Loss: 0.14074797088475635, Validation Loss: 0.7536989632685474\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 300 epochs, Accuracy: 0.8054\n",
        "\n",
        "Comparing the modified CNN and ResNet outputs:\n",
        "\n",
        "Accuracy:\n",
        "\n",
        "Modified CNN: Accuracy of 76.1%\n",
        "ResNet: Accuracy of 80.54%\n",
        "ResNet achieved a higher accuracy compared to the modified CNN.\n",
        "\n",
        "Training Loss:\n",
        "\n",
        "Modified CNN: Loss of 0.0876\n",
        "ResNet: Loss of 0.1407\n",
        "The modified CNN has a lower training loss compared to ResNet.\n",
        "\n",
        "Validation Loss:\n",
        "\n",
        "Modified CNN: Validation loss of 1.3733\n",
        "ResNet: Validation loss of 0.7537\n",
        "ResNet has a significantly lower validation loss, indicating better generalization.\n",
        "\n",
        "Training Time:\n",
        "\n",
        "Both models were trained for 300 epochs, and early stopping was triggered after 10 epochs without improvement.\n",
        "In summary, while the modified CNN performed well, ResNet achieved higher accuracy and lower validation loss, suggesting that ResNet has better overall performance in this scenario."
      ],
      "metadata": {
        "id": "RwmG8FWBfhad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "b. Perform three additional training and evaluations for your ResNet-10 to assess the impacts of regularization on your ResNet-10.\n",
        "* ﻿﻿Weight Decay with lambda of 0.001\n",
        "* ﻿﻿Dropout with p=0.3\n",
        "* ﻿﻿Batch Normalization\n",
        "Report and compare your training time, training loss, and evaluation accuracy after 300 epochs across these three different pieces of training.\n",
        "Analyze your results in your report and compare them against problem 1. On training time, you achieved accuracy."
      ],
      "metadata": {
        "id": "KfX4L0HLBtag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the Residual Block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out += self.downsample(identity)\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# Define the ResNet-10 architecture\n",
        "class ResNet10(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ResNet10, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(16, 16, num_blocks=2, stride=1)\n",
        "        self.layer2 = self.make_layer(16, 32, num_blocks=2, stride=2)\n",
        "        self.layer3 = self.make_layer(32, 64, num_blocks=2, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride))\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, stride=1))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Function to train and evaluate the model with early stopping\n",
        "def train_and_evaluate_with_early_stopping(model, criterion, optimizer, num_epochs=20, regularization_name=\"None\", early_stopping_threshold=10):\n",
        "    print(f\"Training ResNet-10 with {regularization_name} regularization and early stopping:\")\n",
        "\n",
        "    best_validation_loss = float('inf')\n",
        "    early_stopping_counter = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        validation_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                validation_loss += criterion(outputs, labels).item()\n",
        "\n",
        "        # Print and check for early stopping\n",
        "        average_loss = total_loss / len(train_loader)\n",
        "        average_validation_loss = validation_loss / len(test_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {average_loss}, Validation Loss: {average_validation_loss}\")\n",
        "\n",
        "        if average_validation_loss < best_validation_loss:\n",
        "            best_validation_loss = average_validation_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "\n",
        "        if early_stopping_counter >= early_stopping_threshold:\n",
        "            print(f\"Early stopping after {early_stopping_threshold} epochs without improvement.\")\n",
        "            break\n",
        "\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Training Time: {epoch + 1} epochs, Accuracy: {accuracy}\\n\")\n",
        "\n",
        "# Initialize the ResNet-10 model, loss function, and optimizer for each scenario\n",
        "model_no_regularization = ResNet10(num_classes=10).to(device)\n",
        "model_weight_decay = ResNet10(num_classes=10).to(device)\n",
        "model_dropout = ResNet10(num_classes=10).to(device)\n",
        "model_batch_norm = ResNet10(num_classes=10).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# # Training loop for ResNet-10 without regularization with early stopping\n",
        "# optimizer_no_regularization = optim.Adam(model_no_regularization.parameters(), lr=0.001, weight_decay=0)\n",
        "# train_and_evaluate_with_early_stopping(model_no_regularization, criterion, optimizer_no_regularization, num_epochs=300, regularization_name=\"No Regularization\")\n",
        "\n",
        "# # Training loop for ResNet-10 with Weight Decay (L2 regularization) with early stopping\n",
        "# optimizer_weight_decay = optim.Adam(model_weight_decay.parameters(), lr=0.001, weight_decay=0.001)\n",
        "# train_and_evaluate_with_early_stopping(model_weight_decay, criterion, optimizer_weight_decay, num_epochs=300, regularization_name=\"Weight Decay\")\n",
        "\n",
        "# Training loop for ResNet-10 with Dropout with early stopping\n",
        "model_dropout.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(64, 10)).to(device)  # Add dropout to the fully connected layer\n",
        "optimizer_dropout = optim.Adam(model_dropout.parameters(), lr=0.01)\n",
        "train_and_evaluate_with_early_stopping(model_dropout, criterion, optimizer_dropout, num_epochs=20, regularization_name=\"Dropout\")\n",
        "\n",
        "# Training loop for ResNet-10 with Batch Normalization with early stopping\n",
        "optimizer_batch_norm = optim.Adam(model_batch_norm.parameters(), lr=0.01)\n",
        "train_and_evaluate_with_early_stopping(model_batch_norm, criterion, optimizer_batch_norm, num_epochs=20, regularization_name=\"Batch Normalization\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz4S-XI8alfH",
        "outputId": "86f6e566-4760-4aa9-816a-1c46bd3d214e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Training ResNet-10 with Dropout regularization and early stopping:\n",
            "Epoch 1/20, Loss: 1.5658981261198477, Validation Loss: 1.2364503786822034\n",
            "Epoch 2/20, Loss: 1.1134927833781523, Validation Loss: 0.9994412189835955\n",
            "Epoch 3/20, Loss: 0.911093364705515, Validation Loss: 0.7939224448173668\n",
            "Epoch 4/20, Loss: 0.7742542710984149, Validation Loss: 0.7534522323092078\n",
            "Epoch 5/20, Loss: 0.6779960084068196, Validation Loss: 0.8197496548579757\n",
            "Epoch 6/20, Loss: 0.6086168395512549, Validation Loss: 0.710800096107896\n",
            "Epoch 7/20, Loss: 0.5524985785679439, Validation Loss: 0.6717706421378312\n",
            "Epoch 8/20, Loss: 0.5010968905390071, Validation Loss: 0.5725557973999886\n",
            "Epoch 9/20, Loss: 0.4592542575905695, Validation Loss: 0.6131337188231717\n",
            "Epoch 10/20, Loss: 0.4274180505967811, Validation Loss: 0.5628867433139473\n",
            "Epoch 11/20, Loss: 0.3857717586066717, Validation Loss: 0.6865508981571076\n",
            "Epoch 12/20, Loss: 0.35878281939365064, Validation Loss: 0.6983868670501526\n",
            "Epoch 13/20, Loss: 0.33270979098156284, Validation Loss: 0.610032485359034\n",
            "Epoch 14/20, Loss: 0.310820110189869, Validation Loss: 0.6006922195101999\n",
            "Epoch 15/20, Loss: 0.29095282035944103, Validation Loss: 0.6680005196553127\n",
            "Epoch 16/20, Loss: 0.26724531094703224, Validation Loss: 0.6569304834505555\n",
            "Epoch 17/20, Loss: 0.24539192766903917, Validation Loss: 0.6592549377945578\n",
            "Epoch 18/20, Loss: 0.23863645432435948, Validation Loss: 0.6929080670426606\n",
            "Epoch 19/20, Loss: 0.21682897155337474, Validation Loss: 0.6788160557010371\n",
            "Epoch 20/20, Loss: 0.20441421747798352, Validation Loss: 0.6777726767738913\n",
            "Early stopping after 10 epochs without improvement.\n",
            "Training Time: 20 epochs, Accuracy: 0.8161\n",
            "\n",
            "Training ResNet-10 with Batch Normalization regularization and early stopping:\n",
            "Epoch 1/20, Loss: 1.518590752044907, Validation Loss: 1.234613559428294\n",
            "Epoch 2/20, Loss: 1.001643056073762, Validation Loss: 0.9124549145151855\n",
            "Epoch 3/20, Loss: 0.7707557068837573, Validation Loss: 0.7683630703361171\n",
            "Epoch 4/20, Loss: 0.6490675917900431, Validation Loss: 0.6411122212744063\n",
            "Epoch 5/20, Loss: 0.5647438241697639, Validation Loss: 0.6016352653123771\n",
            "Epoch 6/20, Loss: 0.5044423445220798, Validation Loss: 0.6131676496214168\n",
            "Epoch 7/20, Loss: 0.4482049181714387, Validation Loss: 0.6135396211390283\n",
            "Epoch 8/20, Loss: 0.3927748886977925, Validation Loss: 0.70230253012317\n",
            "Epoch 9/20, Loss: 0.3519867124879147, Validation Loss: 0.5942548887365183\n",
            "Epoch 10/20, Loss: 0.3124762815244667, Validation Loss: 0.6645467023181307\n",
            "Epoch 11/20, Loss: 0.28198050917185785, Validation Loss: 0.587647859078304\n",
            "Epoch 12/20, Loss: 0.2477325106616063, Validation Loss: 0.6313081201474378\n",
            "Epoch 13/20, Loss: 0.22085644737305238, Validation Loss: 0.728734579625403\n",
            "Epoch 14/20, Loss: 0.19646720500076975, Validation Loss: 0.7158429899792762\n",
            "Epoch 15/20, Loss: 0.17457799996008805, Validation Loss: 0.8089482476756831\n",
            "Epoch 16/20, Loss: 0.16354455179094202, Validation Loss: 0.7860335031892084\n",
            "Epoch 17/20, Loss: 0.14644232364204687, Validation Loss: 0.7909892945532586\n",
            "Epoch 18/20, Loss: 0.13618721975289913, Validation Loss: 0.7906098917696127\n",
            "Epoch 19/20, Loss: 0.11962778670022554, Validation Loss: 0.8247421537614932\n",
            "Epoch 20/20, Loss: 0.11434049572071532, Validation Loss: 0.8762146424336038\n",
            "Training Time: 20 epochs, Accuracy: 0.8019\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training ResNet-10 with No Regularization regularization:\n",
        "\n",
        "Epoch 11/300, Loss: 0.3103035500516062\n",
        "Validation Loss: 0.6360743996823669\n",
        "Early stopping at epoch 11 as validation loss did not improve for 5 epochs.\n",
        "Training Time: 300 epochs, Accuracy: 0.7985\n",
        "\n",
        "Training loop for ResNet-10 with Weight Decay (L2 regularization) with early stopping\n",
        "\n",
        "Epoch 20/20, Loss: 0.20441421747798352, Validation Loss: 0.6777726767738913\n",
        "Early stopping after 10 epochs without improvement.\n",
        "Training Time: 20 epochs, Accuracy: 0.8161\n",
        "\n",
        "\n",
        "Training ResNet-10 with Batch Normalization regularization and early stopping:\n",
        "\n",
        "Epoch 15/15, Loss: 0.16131393375384914, Validation Loss: 0.7098411087207733\n",
        "Training Time: 15 epochs, Accuracy: 0.8099"
      ],
      "metadata": {
        "id": "qhU8p7GxvWyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGWpujkfaldG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9_fkKwUalak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RaKg2NUMalX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cbmxiqmealV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCAlKnG_alS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zU_ZxtrHalQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R7Zlm5D9alN3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}