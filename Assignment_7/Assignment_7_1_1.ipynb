{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODQt7Nzt1WohcZ8Ko0osNn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubymanderna/ML_ECGR5105/blob/main/Assignment_7/Assignment_7_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1 (50pts): a. Build a Convolutional Neural Network, like what we built in lectures to classify the images across all 10 classes in CIFAR 10. You need to adjust the fully connected layer at the end properly concerning the number of output classes. Train your network for 300 epochs. Report your training time, training loss, and evaluation accuracy after 300 epochs. Analyze your results in your report and compare them against a fully connected network (homework 2) on training time, achieved accuracy, and model size. Make sure to submit your code by providing the GitHub URL of your course repository for this course."
      ],
      "metadata": {
        "id": "1Ic_WpSywB8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set the device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = SimpleCNN(num_classes=10).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stop_threshold = 8\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 300\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            val_loss += criterion(outputs, labels).item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(test_loader)\n",
        "    print(f\"Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Check for early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        epochs_no_improve = 0\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve == early_stop_threshold:\n",
        "        print(f\"Early stopping at epoch {epoch + 1} as validation loss did not improve for {early_stop_threshold} epochs.\")\n",
        "        break\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Training Time: {num_epochs} epochs, Accuracy: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbGFGY4IxHmA",
        "outputId": "4756e8b0-3402-4fc5-d142-b55aa86883e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 1.3487361193160572\n",
            "Validation Loss: 1.0727805468686826\n",
            "Epoch 2/300, Loss: 0.9653622771772887\n",
            "Validation Loss: 0.9074631401687671\n",
            "Epoch 3/300, Loss: 0.8127234652447883\n",
            "Validation Loss: 0.8747242848584607\n",
            "Epoch 4/300, Loss: 0.7022764265461041\n",
            "Validation Loss: 0.8547485698560241\n",
            "Epoch 5/300, Loss: 0.6065885352966426\n",
            "Validation Loss: 0.8055725946168232\n",
            "Epoch 6/300, Loss: 0.5136076732326651\n",
            "Validation Loss: 0.8168383269173325\n",
            "Epoch 7/300, Loss: 0.43465331821795317\n",
            "Validation Loss: 0.8703805582158884\n",
            "Epoch 8/300, Loss: 0.3576720280339346\n",
            "Validation Loss: 0.9364019002124762\n",
            "Epoch 9/300, Loss: 0.2886040548763007\n",
            "Validation Loss: 0.9868400444270699\n",
            "Epoch 10/300, Loss: 0.23224034644377506\n",
            "Validation Loss: 1.0660690604501468\n",
            "Epoch 11/300, Loss: 0.18692649032472802\n",
            "Validation Loss: 1.184711794754502\n",
            "Epoch 12/300, Loss: 0.1476428548726813\n",
            "Validation Loss: 1.3301054622716963\n",
            "Epoch 13/300, Loss: 0.12381619647088106\n",
            "Validation Loss: 1.4432452017334616\n",
            "Early stopping at epoch 13 as validation loss did not improve for 8 epochs.\n",
            "Training Time: 300 epochs, Accuracy: 0.7163\n"
          ]
        }
      ]
    }
  ]
}