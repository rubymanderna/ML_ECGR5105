{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1HScGWEcA5hf1dUWxoVihupKy6w-m-mAI",
      "authorship_tag": "ABX9TyNUXpLEy433EKoQGyNu1q35",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubymanderna/ML_ECGR5105/blob/main/Homework_5/Homework_5_2_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2 (40 pts):\n",
        "\n",
        "2.a. Develop preprocessing and a training loop to train a linear regression model that predicts housing price based on the following input variables:\n",
        "\n",
        "area, bedrooms, bathrooms, stories, parking\n",
        "\n",
        "For this, you need to use the housing dataset. For training and validation, use 80% (training) and 20% (validation) split. Identify the best parameters for your linear regression model based on the above input variables. In this case, you will have six parameters:\n",
        "\n",
        "2.b Use 5000 epochs for your training. Explore different learning rates from 0.1 to 0.0001 (you need four separate trainings). Report your loss and validation accuracy for every 500 epochs per training. Pick the best linear model.\n",
        "\n",
        "2.c. Compare your results against the linear regression done in homework 1. Do you see meaningful differences?"
      ],
      "metadata": {
        "id": "iq2vnzRdEXvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDatasetfrom sklearn.linear_model import LinearRegression\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/ML_ECGR5105/Housing.csv')"
      ],
      "metadata": {
        "id": "_-CeG_1PEvE-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNUXBHFxEvCn",
        "outputId": "a0a5a5e5-cc4c-470e-a191-901f3619d46f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['price', 'area', 'bedrooms', 'bathrooms', 'stories', 'mainroad',\n",
              "       'guestroom', 'basement', 'hotwaterheating', 'airconditioning',\n",
              "       'parking', 'prefarea', 'furnishingstatus'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Select relevant features\n",
        "selected_features = ['area', 'bedrooms', 'bathrooms', 'stories', 'parking', 'price']\n",
        "df = df[selected_features]\n",
        "\n",
        "# Train-Validation Split\n",
        "X = df[['area', 'bedrooms', 'bathrooms', 'stories', 'parking']]\n",
        "y = df['price']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the Input Features\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define a simple linear regression model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Linear Regression Model\n",
        "input_size = X_train.shape[1]\n",
        "model = LinearRegressionModel(input_size)\n",
        "\n",
        "# Training Loop with SGD optimizer and tracking best model\n",
        "n_epochs = 5000\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "best_model_sgd = None\n",
        "best_mse_sgd = float('inf')\n",
        "\n",
        "print(\"Training with SGD optimizer:\")\n",
        "for lr in learning_rates:\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    optimizer_sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_sgd.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_sgd.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            # Validation\n",
        "            y_pred_val = model(X_val_tensor)\n",
        "            mse = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val.detach().numpy())\n",
        "            print(f'Epoch {epoch}, Mean Squared Error (SGD): {mse}')\n",
        "\n",
        "            # Track the best model based on validation loss for SGD\n",
        "            if mse < best_mse_sgd:\n",
        "                best_mse_sgd = mse\n",
        "                best_model_sgd = model.state_dict()\n",
        "\n",
        "# Load the best SGD model\n",
        "model.load_state_dict(best_model_sgd)\n",
        "\n",
        "# Test the best SGD model on the validation set\n",
        "y_pred_val_sgd = model(X_val_tensor)\n",
        "mse_sgd = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val_sgd.detach().numpy())\n",
        "print(f'\\nBest Model (SGD) - Mean Squared Error on Validation Set: {mse_sgd}')\n",
        "\n",
        "\n",
        "# Training Loop with Adam optimizer and tracking best model\n",
        "best_model_adam = None\n",
        "best_mse_adam = float('inf')\n",
        "\n",
        "print(\"\\nTraining with Adam optimizer:\")\n",
        "for lr in learning_rates:\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    optimizer_adam = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_adam.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_adam.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            # Validation\n",
        "            y_pred_val = model(X_val_tensor)\n",
        "            mse = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val.detach().numpy())\n",
        "            print(f'Epoch {epoch}, Mean Squared Error (Adam): {mse}')\n",
        "\n",
        "            # Track the best model based on validation loss for Adam\n",
        "            if mse < best_mse_adam:\n",
        "                best_mse_adam = mse\n",
        "                best_model_adam = model.state_dict()\n",
        "\n",
        "# Load the best Adam model\n",
        "model.load_state_dict(best_model_adam)\n",
        "\n",
        "# Test the best Adam model on the validation set\n",
        "y_pred_val_adam = model(X_val_tensor)\n",
        "mse_adam = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val_adam.detach().numpy())\n",
        "print(f'\\nBest Model (Adam) - Mean Squared Error on Validation Set: {mse_adam}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edxIdpgmD8bh",
        "outputId": "9568b0f1-9290-41ee-9306-a5bbe44b9219"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGD optimizer:\n",
            "Learning rate: 0.1\n",
            "Epoch 0, Mean Squared Error (SGD): 2494470815744.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2359427334144.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2330197491712.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2265070174208.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2254688223232.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2173691756544.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2311604666368.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2301735993344.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2234002964480.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2350491893760.0\n",
            "Learning rate: 0.01\n",
            "Epoch 0, Mean Squared Error (SGD): 2340860723200.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2301604921344.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2300179906560.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2303374131200.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2289769119744.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2295129702400.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2291934429184.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2298051559424.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2296874270720.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2303108841472.0\n",
            "Learning rate: 0.001\n",
            "Epoch 0, Mean Squared Error (SGD): 2301001990144.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2292120813568.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2295385817088.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2291834552320.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2293441757184.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2293547139072.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2291899826176.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2293848604672.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2294229499904.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2291605176320.0\n",
            "Learning rate: 0.0001\n",
            "Epoch 0, Mean Squared Error (SGD): 2293234663424.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2292457144320.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2292410220544.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2292077035520.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2292947091456.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2293063483392.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2292229341184.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2292510883840.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2292236943360.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2292300906496.0\n",
            "\n",
            "Best Model (SGD) - Mean Squared Error on Validation Set: 2292684947456.0\n",
            "\n",
            "Training with Adam optimizer:\n",
            "Learning rate: 0.1\n",
            "Epoch 0, Mean Squared Error (Adam): 2292686258176.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292684423168.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292684685312.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292684423168.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292686782464.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292684423168.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292682850304.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292682850304.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292679704576.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292679442432.0\n",
            "Learning rate: 0.01\n",
            "Epoch 0, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Learning rate: 0.001\n",
            "Epoch 0, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Learning rate: 0.0001\n",
            "Epoch 0, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292676820992.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292676820992.0\n",
            "\n",
            "Best Model (Adam) - Mean Squared Error on Validation Set: 2292676820992.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Best model**"
      ],
      "metadata": {
        "id": "RlIhhkCyM1w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Model (SGD) - Mean Squared Error on Validation Set: 2292684947456.0\n",
        "\n",
        "Best Model (Adam) - Mean Squared Error on Validation Set: 2292676820992.0\n"
      ],
      "metadata": {
        "id": "bA3v5mTRMvo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*2*.c. Compare your results against the linear regression done in homework 1. Do you see meaningful differences?\n",
        "\n",
        "\n",
        "homework 2 result\n",
        "\n",
        "**For input normalization results for 3b Added Penality**\n",
        "\n",
        "Best Learning Rate: 0.01\n",
        "Best MSE: 900238626517.4305\n",
        "\n",
        "**For input normalization results for 2b without Penality**\n",
        "\n",
        "Best Learning Rate: 0.1\n",
        "Best MSE: 894200375157.158\n",
        "\n",
        "\n",
        "After comparing with homework 2 model with ADAM optimizer is performing better as it has min loss"
      ],
      "metadata": {
        "id": "0yHhFZXiUkMg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 3 (30 pts):\n",
        "\n",
        "Repeat all sections of problem 2 using all the input features from the housing price dataset."
      ],
      "metadata": {
        "id": "vz1TPDCtU1qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X = df.drop('price', axis=1)\n",
        "y = df['price']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the Input Features\n",
        "scaler = StandardScaler()\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Create DataLoader for training set\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Define a simple linear regression model\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Linear Regression Model\n",
        "input_size = X_train.shape[1]\n",
        "model = LinearRegressionModel(input_size)\n",
        "\n",
        "# Training Loop with SGD optimizer and tracking best model\n",
        "n_epochs = 5000\n",
        "learning_rates = [0.1, 0.01, 0.001, 0.0001]\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "best_model_sgd = None\n",
        "best_mse_sgd = float('inf')\n",
        "\n",
        "print(\"Training with SGD optimizer:\")\n",
        "for lr in learning_rates:\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    optimizer_sgd = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_sgd.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_sgd.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            # Validation\n",
        "            y_pred_val = model(X_val_tensor)\n",
        "            mse = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val.detach().numpy())\n",
        "            print(f'Epoch {epoch}, Mean Squared Error (SGD): {mse}')\n",
        "\n",
        "            # Track the best model based on validation loss for SGD\n",
        "            if mse < best_mse_sgd:\n",
        "                best_mse_sgd = mse\n",
        "                best_model_sgd = model.state_dict()\n",
        "\n",
        "# Load the best SGD model\n",
        "model.load_state_dict(best_model_sgd)\n",
        "\n",
        "# Test the best SGD model on the validation set\n",
        "y_pred_val_sgd = model(X_val_tensor)\n",
        "mse_sgd = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val_sgd.detach().numpy())\n",
        "print(f'\\nBest Model (SGD) - Mean Squared Error on Validation Set: {mse_sgd}')\n",
        "\n",
        "\n",
        "# Training Loop with Adam optimizer and tracking best model\n",
        "best_model_adam = None\n",
        "best_mse_adam = float('inf')\n",
        "\n",
        "print(\"\\nTraining with Adam optimizer:\")\n",
        "for lr in learning_rates:\n",
        "    print(f\"Learning rate: {lr}\")\n",
        "    optimizer_adam = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        for batch_X, batch_y in train_loader:\n",
        "            # Forward pass\n",
        "            y_pred = model(batch_X)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(y_pred, batch_y)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            optimizer_adam.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer_adam.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            # Validation\n",
        "            y_pred_val = model(X_val_tensor)\n",
        "            mse = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val.detach().numpy())\n",
        "            print(f'Epoch {epoch}, Mean Squared Error (Adam): {mse}')\n",
        "\n",
        "            # Track the best model based on validation loss for Adam\n",
        "            if mse < best_mse_adam:\n",
        "                best_mse_adam = mse\n",
        "                best_model_adam = model.state_dict()\n",
        "\n",
        "# Load the best Adam model\n",
        "model.load_state_dict(best_model_adam)\n",
        "\n",
        "# Test the best Adam model on the validation set\n",
        "y_pred_val_adam = model(X_val_tensor)\n",
        "mse_adam = mean_squared_error(y_val_tensor.detach().numpy(), y_pred_val_adam.detach().numpy())\n",
        "print(f'\\nBest Model (Adam) - Mean Squared Error on Validation Set: {mse_adam}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30XuMdbaEu96",
        "outputId": "482fb3df-417e-4a99-92e9-f0655505ed4f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with SGD optimizer:\n",
            "Learning rate: 0.1\n",
            "Epoch 0, Mean Squared Error (SGD): 2436422696960.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2426676969472.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2335065767936.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2207368347648.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2459445493760.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2501334007808.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2195636617216.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2225410408448.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2304953286656.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2290366021632.0\n",
            "Learning rate: 0.01\n",
            "Epoch 0, Mean Squared Error (SGD): 2244973690880.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2291586301952.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2293496807424.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2302454267904.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2302360944640.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2297196183552.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2285594738688.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2289809752064.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2283516985344.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2305165885440.0\n",
            "Learning rate: 0.001\n",
            "Epoch 0, Mean Squared Error (SGD): 2289995350016.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2292751007744.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2291931545600.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2291071451136.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2293649899520.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2292804747264.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2292689141760.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2293072396288.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2293586722816.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2293167030272.0\n",
            "Learning rate: 0.0001\n",
            "Epoch 0, Mean Squared Error (SGD): 2289936105472.0\n",
            "Epoch 500, Mean Squared Error (SGD): 2292008878080.0\n",
            "Epoch 1000, Mean Squared Error (SGD): 2292757823488.0\n",
            "Epoch 1500, Mean Squared Error (SGD): 2292606566400.0\n",
            "Epoch 2000, Mean Squared Error (SGD): 2292422541312.0\n",
            "Epoch 2500, Mean Squared Error (SGD): 2293095202816.0\n",
            "Epoch 3000, Mean Squared Error (SGD): 2292837777408.0\n",
            "Epoch 3500, Mean Squared Error (SGD): 2293173321728.0\n",
            "Epoch 4000, Mean Squared Error (SGD): 2292583235584.0\n",
            "Epoch 4500, Mean Squared Error (SGD): 2292249264128.0\n",
            "\n",
            "Best Model (SGD) - Mean Squared Error on Validation Set: 2292845641728.0\n",
            "\n",
            "Training with Adam optimizer:\n",
            "Learning rate: 0.1\n",
            "Epoch 0, Mean Squared Error (Adam): 2292845641728.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292845117440.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292844855296.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292845117440.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292844855296.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292845117440.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292846166016.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292849573888.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292852719616.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292853768192.0\n",
            "Learning rate: 0.01\n",
            "Epoch 0, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292854292480.0\n",
            "Learning rate: 0.001\n",
            "Epoch 0, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Learning rate: 0.0001\n",
            "Epoch 0, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 1500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 2500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 3500, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4000, Mean Squared Error (Adam): 2292854554624.0\n",
            "Epoch 4500, Mean Squared Error (Adam): 2292854554624.0\n",
            "\n",
            "Best Model (Adam) - Mean Squared Error on Validation Set: 2292854554624.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BEST MODEL\n",
        "\n"
      ],
      "metadata": {
        "id": "yu0XOQSSOFTN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best Model (SGD) - Mean Squared Error on Validation Set: 2292845641728.0\n",
        "\n",
        "Best Model (Adam) - Mean Squared Error on Validation Set: 2292854554624.0\n",
        "\n",
        "Best model with ADAM optimizer performed better"
      ],
      "metadata": {
        "id": "X22lZho4OmGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "homework 2 result\n",
        "\n",
        "**For input normalization results for 3b Added Penality**\n",
        "\n",
        "Best Learning Rate: 0.01\n",
        "Best MSE: 900238626517.4305\n",
        "\n",
        "**For input normalization results for 2b without Penality**\n",
        "\n",
        "Best Learning Rate: 0.1\n",
        "Best MSE: 894200375157.158\n",
        "\n",
        "After comparing the results model with the ADAM optimizer is performing better and as min loss"
      ],
      "metadata": {
        "id": "lcQZyNIHQh4B"
      }
    }
  ]
}