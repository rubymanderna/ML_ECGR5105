{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1mr-YFi3a1xE324n-eTewH5DjgmWs1TIm",
      "authorship_tag": "ABX9TyP0KZYYnctON379vi9wL1b8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubymanderna/ML_ECGR5105/blob/main/Homework_3_ML/Homework_3_4_and_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 4 (20pts):\n",
        "\n",
        "Use the cancer dataset to build a logistic regression model to classify the type of cancer (Malignant vs. benign). Use the PCA feature extraction for your training. Perform N number of independent training (N=1, â€¦, K). Identify the optimum number of K, principal components that achieve the highest classification accuracy. Report your classification accuracy, precision, recall, and F1 score over a different number of Ks. Explain and elaborate on your results and compare them against problems 2 and 3."
      ],
      "metadata": {
        "id": "PT0OhEXs-iYW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9agamVqC-fW6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/ML_ECGR5105/cancer.csv\")\n",
        "print(df.columns)\n",
        "# Step 1: Data Preprocessing\n",
        "df = df.drop(columns=['id', 'Unnamed: 32'])\n",
        "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afaUqqpP-wWm",
        "outputId": "c6c5b72d-743f-499f-f60e-9030f5b1cc3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n",
            "       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n",
            "       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n",
            "       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n",
            "       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n",
            "       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n",
            "       'perimeter_worst', 'area_worst', 'smoothness_worst',\n",
            "       'compactness_worst', 'concavity_worst', 'concave points_worst',\n",
            "       'symmetry_worst', 'fractal_dimension_worst', 'Unnamed: 32'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "X = df.drop(columns=['diagnosis'])\n",
        "y = df['diagnosis']\n",
        "\n",
        "\n",
        "# Normalize the features (min-max scaling)\n",
        "scaler = MinMaxScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Initialize variables\n",
        "best_accuracy = 0\n",
        "best_k = 0\n",
        "\n",
        "# Loop for different numbers of Principal Components (K)\n",
        "for k in range(1, 11):  # Adjust range as needed\n",
        "    # Step 3: PCA\n",
        "    pca = PCA(n_components=k)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Step 4: Model Training\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Step 5: Performance Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Store metrics\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k = k\n",
        "\n",
        "    # Report metrics for each K\n",
        "    print(f'K = {k}: Accuracy = {accuracy}, Precision = {precision}, Recall = {recall}, F1 Score = {f1}')\n",
        "\n",
        "# Step 6: Identify Optimum K\n",
        "print(f'Best K: {best_k}, Best Accuracy: {best_accuracy}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kA4VNR-Q-wUg",
        "outputId": "1f3baed9-3c69-4e59-83a5-1874e0665e79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K = 1: Accuracy = 0.9473684210526315, Precision = 0.9743589743589743, Recall = 0.8837209302325582, F1 Score = 0.9268292682926831\n",
            "K = 2: Accuracy = 0.9649122807017544, Precision = 1.0, Recall = 0.9069767441860465, F1 Score = 0.951219512195122\n",
            "K = 3: Accuracy = 0.9736842105263158, Precision = 0.9761904761904762, Recall = 0.9534883720930233, F1 Score = 0.9647058823529412\n",
            "K = 4: Accuracy = 0.9649122807017544, Precision = 0.975609756097561, Recall = 0.9302325581395349, F1 Score = 0.9523809523809524\n",
            "K = 5: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "K = 6: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "K = 7: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "K = 8: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "K = 9: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "K = 10: Accuracy = 0.9824561403508771, Precision = 1.0, Recall = 0.9534883720930233, F1 Score = 0.9761904761904763\n",
            "Best K: 5, Best Accuracy: 0.9824561403508771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5 (20pts):\n",
        "\n",
        "Can you repeat problem 4? This time, replace the logistic regression classifier with the Bayes classifier. Report your results (classification accuracy, precision, recall and F1 score). Compare your results against problems 2, 3 and 4."
      ],
      "metadata": {
        "id": "m1l5QXVRExDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Initialize variables\n",
        "best_accuracy = 0\n",
        "best_k = 0\n",
        "\n",
        "# Assuming X is your feature matrix\n",
        "scaler = MinMaxScaler()  # Or StandardScaler\n",
        "X_normalized = scaler.fit_transform(X)\n",
        "\n",
        "# Loop for different numbers of Principal Components (K)\n",
        "for k in range(1, 11):  # Adjust range as needed\n",
        "    # Step 3: PCA\n",
        "\n",
        "    pca = PCA(n_components=k)\n",
        "    X_pca = pca.fit_transform(X_normalized)\n",
        "\n",
        "    # Step 4: Naive Bayes Classifier (Gaussian)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "    model = GaussianNB()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Step 5: Performance Metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "     # Store metrics\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k = k\n",
        "\n",
        "    # Report metrics for each K\n",
        "    print(f'K = {k}: Accuracy = {accuracy}, Precision = {precision}, Recall = {recall}, F1 Score = {f1}')\n",
        "\n",
        "# Step 6: Identify Optimum K\n",
        "print(f'Best K: {best_k}, Best Accuracy: {best_accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9X_gaU5-wSQ",
        "outputId": "c8c5d44c-a22d-4fbc-c75b-e5d4e94bdd1f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K = 1: Accuracy = 0.956140350877193, Precision = 0.975, Recall = 0.9069767441860465, F1 Score = 0.9397590361445783\n",
            "K = 2: Accuracy = 0.956140350877193, Precision = 0.975, Recall = 0.9069767441860465, F1 Score = 0.9397590361445783\n",
            "K = 3: Accuracy = 0.956140350877193, Precision = 0.975, Recall = 0.9069767441860465, F1 Score = 0.9397590361445783\n",
            "K = 4: Accuracy = 0.956140350877193, Precision = 0.975, Recall = 0.9069767441860465, F1 Score = 0.9397590361445783\n",
            "K = 5: Accuracy = 0.956140350877193, Precision = 0.975, Recall = 0.9069767441860465, F1 Score = 0.9397590361445783\n",
            "K = 6: Accuracy = 0.9473684210526315, Precision = 0.9512195121951219, Recall = 0.9069767441860465, F1 Score = 0.9285714285714286\n",
            "K = 7: Accuracy = 0.9473684210526315, Precision = 0.9302325581395349, Recall = 0.9302325581395349, F1 Score = 0.9302325581395349\n",
            "K = 8: Accuracy = 0.9473684210526315, Precision = 0.9302325581395349, Recall = 0.9302325581395349, F1 Score = 0.9302325581395349\n",
            "K = 9: Accuracy = 0.9473684210526315, Precision = 0.9302325581395349, Recall = 0.9302325581395349, F1 Score = 0.9302325581395349\n",
            "K = 10: Accuracy = 0.9385964912280702, Precision = 0.9285714285714286, Recall = 0.9069767441860465, F1 Score = 0.9176470588235294\n",
            "Best K: 1, Best Accuracy: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explaination for 4 and 5**"
      ],
      "metadata": {
        "id": "AtdruSU8dXvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Final result from problem 2**\n",
        "\n",
        "**After adding weight penalty**\n",
        "\n",
        "Final Accuracy: 0.9736842105263158\n",
        "Final Precision: 1.0\n",
        "Final Recall: 0.9302325581395349\n",
        "Final F1 Score: 0.963855421686747\n",
        "Confusion Matrix: [[71 0]\n",
        "                                 [ 3 40]]\n",
        "The addition of a penalty term helped to regularize our model and prevent overfitting up to some extent (very slightly but yes) confusion matrix- in Normalize method we have 0 false positive cases, However, we have False negative increase by 1 but on the other hand true negative decreased by 1, our desired goal for cancer data should be to decrease the number of False negative results, Based on confusion matrix better model was without adding penalties(this is based on cancer scenario assumptions).\n",
        "\n",
        "\n",
        "**final result from problem 3**\n",
        "\n",
        "Accuracy: 0.9649122807017544\n",
        "\n",
        "Precision: 0.975609756097561\n",
        "\n",
        "Recall: 0.9302325581395349\n",
        "\n",
        "F1 Score: 0.9523809523809524\n",
        "\n",
        "compare - in our case logistic regression classifier is prformin better for all the metrics.However,if model is overfitting then using naive bias would be good choice here.\n",
        "\n",
        "**final result from problem 4**\n",
        "\n",
        "Best K : 5\n",
        "\n",
        "Best Accuracy : 0.9824561403508771\n",
        "\n",
        "PCA is a technique used for dimensionality reduction.\n",
        "The first few principal components capture the most variance in the data thus reducing dimensionality of the dataset\n",
        "for this case 5 components will capture the most important information in data.\n",
        "\n",
        "These 5 principal components are linear combinations of the original features. They are selected in such a way that they explain the maximum variance in the dataset.\n",
        "\n",
        "In terms of accuracy PCA performs better and also it helps in reducing dimensionality of the dataset\n",
        "\n",
        "**final result from problem 5**\n",
        "\n",
        "Best K : 1\n",
        "\n",
        "Best Accuracy : 0.956140350877193\n",
        "\n",
        "Clearly the model is not performing good if we refer problem 4,3,2 results in terms of accuracy\n",
        "\n",
        "Also it is suggesting that most of the variance in data is coming from one feature which is kind off not possible.\n"
      ],
      "metadata": {
        "id": "SSZyhTibdUh9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1zl965c-wP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hBDCje2X-wMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}